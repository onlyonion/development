《深入理解Kafka--核心设计与实践原理》朱忠华著

- 生产者
- 消费者
- 主题与分区
- 日志存储
- 监控

## 第1章 初始Kafka
### 1.1 基本概念
### 1.2 安装与配置
### 1.3 生产与消费
### 1.4 服务端参数配置
1. zookeeper.KafkaConnectlisteners
2. broker.id
3. log.dir log.dirs
4. message.max.bytes

## 第2章 生产者
### 2.1 客户端开发
1. 必要的参数设置
2. 消息的发送 发后即忘、同步、异步
3. 序列化
4. 分区器
5. 生产者拦截器 在消息发送前做一些准备工作，如过滤、修改、统计类
  
### 2.2 原理分析
### 2.3 重要的生产者参数
1. acks
2. max.request.size
3. retries, retriy.backoff.ms
4. conpression.type
5. connections.max.idle.ms
6. linger.ms
7. receive.buffer.bytes
8. send.buffer.bytes
9. request.timeout.ms


## 第3章 消费者
### 3.1 消费者与消费机
### 3.2 客户端开发
#### 3.2.1 必要的参数配置
- bootstrap.servers
- group.id
- key.deserializer value.deserializer

#### 3.2.2 订阅主题与分区
#### 3.2.3 反序列化
#### 3.2.4 消息消费
Kafka中的消费是基于拉模式的。
#### 3.2.5 位移提交
#### 3.2.6 控制或关闭消费
#### 3.2.7 指定位移消费
#### 3.2.8 再均衡
再均衡是指**分区的所属权**从一个消费者到另一消费者的行为，它为消费组具备高可用性和伸缩性提供保障，可以既方便安全地删除消费组内的消费者或消费组内添加消费者。

#### 3.2.9 消费者拦截器
#### 3.2.10 多线程实现
#### 3.2.11 重要的消费者参数

## 第4章 主题与分区
### 4.1 主题的管理
### 4.2 初始KafkaAdminClient
### 4.3 分区的管理
#### 4.3.1 优先副本的选举
#### 4.3.2 分区重分配
当集群中新增Broker节点时，只有新创建的主题分区才有可能被分配到这个节点上，之前创建的不会分配到新加入的节点。

创建一个包含主题清单的JSON文件，其次根据主题清单和Broker节点清单生成一份重分配方案，最后根据这份方案执行具体重分配动作。
#### 4.3.3 复制限流
分区重分配的本质在于数据复制，先增加新的副本，然后进行数据同步，最后删除旧的副本来达到最终目的。

#### 4.3.4 修改副本因子
创建主题之后可以修改分区的个数，同样可以修改副本因子（副本数）。

### 4.4 如何选择合适的分区数
1. 性能测试工具
2. 分区数越多吞吐量就越高吗
3. 分区数的上限
4. 考量因素 吞吐量、分区数

## 第5章 日志存储
### 5.1 文件目录布局
为了防止Log过大，Kafka又引入日志分段（LogSegment）的概念，将Log切分为多个LogSegment。

- 主题 Topic
  - 分区 Partition
    - 副本 Replica
      - Log日志
        - LogSegment日志分段
          - .log日志文件
          - .index偏移量索引文件
          - .timeIndex时间戳索引文件
          - 其他文件

向Log中追加消息时时顺序写入的，只有最后一个LogSegment才能执行写入操作。随着消息的不断写入，当activieSegment满足一定条件时，
就需要创建新的activieSegment，之后追加的消息将写入新的activieSegment。

### 5.2 日志格式的演变
### 5.3 日志索引
日志切分条件
1. 日志分段文件的大小 borker端参数 log.segment.bytes=10,737,411,824  1GB

偏移量索引
1. relativeOffset 相对偏移量
2. position 物理地址

时间戳索引
1. timestamp
2. relativeOffset

### 5.4 日志清理
1. 日志删除 基于时间、基于日志文件大小、基于日志起始偏移量
2. 日志压缩

### 5.5 磁盘存储
Kafka依赖于文件系统（更底层地来说是磁盘）来存储和缓存消息。  
顺序写盘的速度不仅比随机写盘的速度快，而且也比随机写内存的速度快。Kafka在设计时采用了**文件追加**的方式来写入消息。

#### 5.5.1 页缓存
操作系统实现的一种主要的磁盘缓存，以此用来减少对磁盘IO的操作。

被修改过的页变成了脏页，操作系统会在合适的时间把脏页中的数据写入磁盘，以保证数据的一致性。

Kafka大量使用了页缓存，这是Kafka实现高吞吐量的重要因素之一。虽然消息都是先被写入页缓存，然后由操作系统负责具体的刷盘任务，
但是Kafka同样提供了同步刷盘及间断性强制刷盘（fsync）的功能。

#### 5.5.2 磁盘IO流程
- 应用层
- 内核层 VFS虚拟文件系统、pageCache、buffer、具体文件系统
- 块层 通用块
- 设备层 磁盘Cache、磁盘

Linux系统IO调度策略
- NOOP，No Operation，最简单的FIFO队列，所有IO请求大致按照先来后到的顺序进行操作
- CFQ，Completely Fair Queuing 按照IO请求的地址进行排序，而不是按照先来后到的顺序进行响应
- DEADLINE 在CFQ的基础上，解决IO请求饿死的极端情况。
- ANTICIPATORY

#### 5.5.3 零拷贝
所谓的零拷贝是指将数据直接从**磁盘文件**复制到**网卡设备**中，而不需要经由应用程序之手。零拷贝大大提高了应用程序的性能，减少了内核和用户模式之间的上下文切换。
Linux sendfile() -> Java FileChannel.transferTo()

## 第6章 深入服务端
### 6.1 协议设计
Kafka自定义了一组基于TCP的二进制协议，只要遵守这组协议的格式，就可以向Kafka发送消息，也可以从Kafka中拉取消息。
每种类型的Request都包含相同结构的协议请求头（RequestHeader）和不同结构的请求体（RequestBody）。

- 协议请求头
  - api_key
  - api_version
  - correlation_id 由客户端指定的一个数字来唯一地标识这次请求id，服务端在处理完请求后页会把同样的coorelation_id写到Response中
  - client_id
- 协议响应头
  - correlation_id

### 6.2 时间轮
Kafka中的时间轮（TimingWheel）是一个存储定时任务的环形队列，底层采用数组实现，数组中的每个元素可以存放一个定时任务表（TimerTaskList）。
TimerTaskList是一个环形的双向链表，链表中的每一项表示的都是定时任务项（TimerTaskEntry），其中封装了真正的定时任务（TimerTask）。

时间轮由多个时间格组成，每个时间格代表当前时间轮的基本时间跨度。
### 6.3 延时操作
### 6.4 控制器
#### 6.4.1  控制器的选举及异常恢复
在Kafka集群中会有一个或多个Broker，其中有一个broker会被选举为控制器（KafkaController），它负责管理整个集群中所有分区和副本的状态。
#### 6.4.2 优雅关闭
使用ControllerShutdown关闭Kafka的优点：
1. 让消息完全同步到磁盘上，在服务下次重新上线时不需要进行日志的恢复操作；
2. ControllerShutdown在关闭服务之前，会对其上的leader服务进行迁移，减少分区的不可用时间。

#### 6.4.3 分区leader的选举
### 6.5 参数解密
#### 6.5.1 broker.id
#### 6.5.2 bootstrap.servers
#### 6.5.3 服务端参数列表

## 第7章 深入客户端
### 7.1 分区分配策略
消费者与主题之间的分区分配策略
#### 7.1.1 RangeAssignor分配策略
按照消费者总数和分区总数进行**整除运算**来获得一个跨度，然后将分区按跨度进行平均分配，以保证分区尽可能均匀地分配给所有的消费者。
#### 7.1.2 RoundRobinAssignor分配策略
将消费组内所有消费者及消费者订阅的所有主题的分区按照**字典序排列**，然后通过**轮询**方式逐个将分区以此分配给每个消费者。
#### 7.1.3 StickAssignor分配策略
#### 7.1.4 自定义分区分配策略

### 7.2 消费者协调器和组协议器
### 7.3 __consumer_offsets剖析
### 7.4 事务
#### 7.4.1 消息传输保障
一般而言，消息中间件的消息传输保障有3个层次
1. at most once 至多一次，消息可能丢失，但不会重复
2. at least once 最少一次，消息绝不会丢失，但可能会重复
3. exactly once 恰好一次，每条消息肯定会被传输一次且仅传输一次

#### 7.4.2 幂等
生产者幂等
#### 7.4.3 事务
幂等性不能跨多个分区运作，而事务可以弥补这个缺陷。事务可以保证对多个分区写入操作的原子性。

应用程序必须提供唯一的transactionId。

## 第8章 可靠性探究
### 8.1 副本剖析
### 8.2 日志同步机制
### 8.3 可靠性分析
1. 副本数
2. 客户端acks
3. 生产者发送模式与重试
4. broker刷盘策略 同步刷盘
5. 客户端位移提交

## 第9章 Kafka应用
### 9.1 命令行工具
### 9.2 KafkaConnect
### 9.3 KafkaMirrorMaker
### 9.4 KafkaStreams

## 第10章 Kafka监控
监控可以为应用提供**运行时的数据**作为依据参考，还可以迅速定位问题，提供预防及**告警**等功能。
集群信息、Broker信息、主题信息、消费组信息。
### 10.1 监控数据的来源
### 10.2 消费滞后
消息堆积是消息中间件的一大特色，消息中间件的流量削峰、冗余存储等功能正式得益于消息中间件的消息堆积能力。
消息堆积是消息滞后的一种表现形式，消息中间件中留存的消息与消费的消息之间的差值即为消息堆积量，也称为消息滞后（Lag）量。
### 10.3 同步失效分区
处于同步失效或功能失效的副本统称为失效副本。包含失效副本的分区也就称为同步失效分区。
### 10.4 监控指标说明
### 10.5 监控模块
- 数据采集
- 数据存储 将采集的原始数据经过一定的预处理后进行存储。基于时间序列的数据库OpenTSDB、Redis、MySQL
- 数据展示

## 第11章 高级应用
### 11.1 过期时间
### 11.2 延时队列
延时小时是指消息被发送后，并不想让消费者立刻获取，而是等待特定的时间后，消费者才能获取这个消息进行消费，延时队列一般也被称为延迟队列。
延时的消息到达目标延迟时间后才能被消费，TTL的消息达到目标超时时间后会被丢弃。
### 11.3 死信队列和重试队列
由于某些原因消息无法被正确地投递，为了确保消息不会被无故地丢弃，一般将其置于一个特殊角色的队列，这个队列一般称为死信队列。
如果消费者在消费时发生了一次，那么就不会对这一次消费进行确认，进而发生回滚消息的操作之后，消息始终会放在队列的顶部，然后不断被处理和回滚，导致队列陷入死循环，解决这个问题，引入回退队列。

死信可以看做消费者不嗯呢该处理收到的消息，也可以看做消费者不想处理收到的消息，还可以看作是不符合处理要求的消息。比如超过既定的重试次数之后将消息投入死信队列。

### 11.4 消息路由
```
RabbitMQ Producer -> Exchange -> queue          -> consume
Kafka    Producer -> Topic    -> consumer group -> consume
```

### 11.5 消息轨迹
### 11.6 消息审计
### 11.7 消息代理
### 11.8 消息中间件选型

## 第12章 Kafkla与Spark的集成
### 12.1 Spark的安装及简单应用
### 12.2 Spark编程模型
### 12.3 Spark的运行结构
### 12.4 Spark Stream简介
### 12.5 Kafka与Spark Streaming的整合
### 12.6 Spark SQL
### 12.7 Structured Streaming
### 12.8 Kafka与Structured Streaming的整合