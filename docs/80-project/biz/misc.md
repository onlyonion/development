http://playground.tensorflow.org/


https://www.bayair.com/ai/


激活函数（activation functions）的目标是，将神经网络非线性化。激活函数是连续的（continuous），且可导的（differential）。

sigmoid是平滑（smoothened）的阶梯函数（step function），可导（differentiable）。sigmoid可以将任何值转换为0~1概率，用于二分类。
tanh，即双曲正切（hyperbolic tangent），类似于幅度增大sigmoid，将输入值转换为-1至1之间。
relu，即Rectified Linear Unit，整流线性单元，激活部分神经元，增加稀疏性，当x小于0时，输出值为0，当x大于0时，输出值为x.



https://studio.azureml.net/ 


http://cf.dui88.com/pages/viewpage.action?pageId=106147397
http://cf.dui88.com/pages/viewpage.action?pageId=107385877
http://cf.dui88.com/pages/viewpage.action?pageId=101538216