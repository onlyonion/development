
## mq

### 消息队列的技术选型？
1. 怎么用消息队列？
2. 为什么用消息队列呢?
3. kafka、activemq、rabbitmq、rocketmq特点和区别？

- activemq
- rabbitmq
- kafka
- rocketmq

### 消息队列的高可用？
### 消息重复？
### 消息丢失？如何保证可靠？
### 消息按顺序消费？
### 消息积压？
### 你怎么设计消息队列？


## cache
文件事件处理器

redis基于reactor模式开发的网络编程模型，file event handler，这个文件事件处理器是单线程的，redis才叫单线程模式。
采用IO多路复用机制同时监听多个socket，根据socket上的事件来选择对应的事件处理器来处理。

4个部分
- 多个socket
- IO多路复用
- 文件事件分派器
- 事件处理器（命令请求处理器、命令回复处理器、连接应答处理器）

多个socket可能并发的产生多个不同的操作，每个操作对应不同的文件事件，但是IO多路复用器会监听多个socket，
将socket放入一个队列中排队，每次从队列中取出一个socket给事件分派器，事件分派器把socket给对应的事件处理器。

### 为什么单线程模型效率这么高？
- 纯内存操作
- 核心是基于非阻塞的IO多路复用
- 单线程反而避免了多线程的频繁上下文切换问题

### Redis过期策略？LRU？
- 如何删除：定期删除 + 惰性删除；默认每隔100ms随机删除；client主动获取key的时候，判断过期时间
- 大量key堆积内存，导致redis内存耗尽？走内存淘汰机制

### Redis内存淘汰机制？

### Redis高并发? 主从机构，一主多从
- 单机几乎不可能QPS超过10w
- 读写分离 master-slave
- redis replication（同步复制、异步复制） -> 主从架构 -> 读写分离 -> 水平扩容支撑读高并发

redis replication 的核心机制
- redis采用异步方式复制数据到slave节点
- 一个master node可以配置多个slave node
- slave node也可以连接其他slave node
- slave node做复制的时候，是不会block master node的正常工作
- slave node复制的时候，也不会block自己的查询操作
- slave node主要用来横向扩容，做读写分离，扩容的slave node也可以提高读的吞吐量

master持久化对主从架构的安全保障意义
- 采用了主从架构，建议必须开启master node的持久化（RDB、AOF）
- 不建议slave node作为master node的数据热备。master宕机重启数据变空，slave node复制了空的master，导致丢失数据
- 即使采用了高可用机制，slave node可以自动接管master node，但是也可能sentinal还没检测到master failure，master node就已经重启

全量复制、增量复制

Redis高可用？主从架构 + 哨兵
- redis高可用架构，叫做故障转移，failover，也可以叫做主备切换
- 哨兵功能 集群监控、消息通知、故障转移、配置中心

哨兵本身也是分布式的，作为一个哨兵集群去运行，互相相同工作
- 故障转移时，判断一个master node是宕机了，需要大部分哨兵同意才行，涉及到了分布式选举的问题
- 即使部分哨兵节点挂掉，哨兵集群还是能正常工作
- 哨兵 + redis主从的部署架构，不会保证数据零丢失，只能保证redis集群高可用

### Redis持久化？如何实现？
- RDB 适合冷备，生成多个文件，可以保证Redis性能比AOF高，fork子进程
- AOF 更好的保证数据不丢失，最多丢失1秒钟的数据；每隔1秒，执行fsync操作，保证os cache中的数据写入磁盘
  - append-only模式写入，文件不容易破坏
  - rewrite，对日志压缩，merge后的日志，适合误删后恢复

### Redis集群部署？
- master + slave + sentinal-cluster 缺点：master节点与slave节点数据时一样的
- redis-cluster 分布式，水平拆分，针对海量数据、高并发、高可用场景
- redis cluser vs. replication + sential

分布式存储的核心算法，数据分布算法
- hash算法 大量缓存重建；某个节点失效，大量hash失效，导致几乎全部请求到达数据库
- 一致性hash 自动缓存迁移 + 虚拟节点（自动负载均衡）
  - 保证一个节点宕机，顺时针找下一个（这个节点负载过于大），部分流量到db
  - 某个节点的负载大，增加虚拟节点，负载均衡
- redis cluster, hash slot算法
  - 有固定的16384个hash slot，对每个key计算CRC16值，然后对16384取模，可以获取key对应的hash slot
  - 每个master都持有部分slot
  - hash slot让node的增加和移除很简单，增加一个master，就将其master的hash slot移动部分过去，减少一个master，就将它的hash slot移动到其他的master上
  - 移动hash slot的成本非常低

Gossip 的特点（优势）
- 扩展性 网络可以允许节点的任意增加和减少，新增加的节点的状态最终会与其他节点一致
- 容错 网络中任何节点的宕机和重启都不会影响 Gossip 消息的传播，Gossip 协议具有天然的分布式系统容错特性
- 去中心化 Gossip 协议不要求任何中心节点，所有节点都可以是对等的，任何一个节点无需知道整个网络状况，只要网络是连通的，任意一个节点就可以把消息散播到全网。
- 一致性收敛 协议中的消息会以一传十、十传百一样的指数级速度在网络中快速传播，因此系统状态的不一致可以在很快的时间内收敛到一致。消息传播速度达到了 logN。
- 简单 Gossip 协议的过程极其简单，实现起来几乎没有太多复杂性。

Gossip 的缺陷，分布式网络中，没有一种完美的解决方案
- 消息的延迟
- 消息冗余

### redis缓存雪崩？
- 事前 redis高可用，避免全盘崩溃
- 事中 本地ehcache缓存 + hystrix **限流 & 降级**，避免mysql被打死。**多级缓存**
- 事后 redis持久化，快速恢复缓存数据

### redis穿透？
- 数据库中不存在，每次都查库
- 过滤key；null缓存

如何保证缓存与数据库双写一致性？
- cache aside pattern
  - 先读缓存，缓存没有，读数据库，放入缓存
  - 更新，删缓存（频繁更新缓存并发开销大、二八定律、懒加载），写数据库
- 先删除缓存，再写数据库，并发仍然会不一致。
- 读写 缓存与数据库 读写异步串行化。读写请求排队，内存任务队列（命令队列），串行化。优化过滤重复的读请求。
- 多服务实例部署请求路由，读写请求路由到不同的节点，仍然会不一致。

保持一致性，牺牲高性能。允许弱一致性，性能会提高。

### redis并发竞争问题？并发？锁？
- 分布式锁，数据版本

### redis怎么部署的？


## 分布式服务
### 为什么要将系统进行拆分？
1. 不拆分，一个大系统几十万行代码，20个人维护一份代码；测试问题；
2. 拆分成20个服务

### 如何进行系统拆分？
- 拆分有很多轮，第一轮；团队继续扩大，拆好的服务继续拆分。如果多人维护一个服务，小于等于3个人维护服务。

### 拆分后不用dubbo可以吗？
- 当然可以。比较复杂，重新实现一套rpc框架。

### dubbo支持哪些通信协议和序列化协议？

|          | 优点                   | 缺点           |
| :------- | :--------------------- | :------------- |
| kryo     | 速度快，序列化后体积小 | 跨语言支持复杂 |
| hessian  |                        |                |
| protobuf | 速度快                 | 需静态编译     |
| java     |                        |                |


### dubbo的支持哪些负载均衡、高可用及动态代理的策略?

### SPI是啥？

### 基于dubbo的服务治理、服务降级以及重试？


### 分布式锁是啥？
- `setnx + lua` 或 `set key value px milliseconds nx`

可能会失效；主从 + 哨兵架构，由于主从之间异步复制，如果主挂掉？
```lua
if redis.call("get", KEY[1]) == ARGV[1] then
    return redis.call("del", KEY[1])
else
    return 0
end
```
- Redlock 集群上锁，避免单机上锁时宕机；假设redis-cluster，5个master实例
  - 获取当前时间戳，单位毫秒
  - 轮流尝试在每个master节点创建锁，过期时间较短，一般就几十毫秒
  - 尝试在大多数节点上建立一个锁，比如5个节点就要求是3个节点（n/2 + 1）
  - 客户端计算建立好锁的时间，如果建立锁的时间小于超时时间，就算建立成功
  - 如果锁建立失败，那么就一次删除这个锁
  - 只要别人建立了一把分布式锁，你就得不断轮询去尝试获取锁
- zookeeper 基于临时节点；基于临时顺序节点
  - 某个节点尝试创建临时节点znode，此时创建成功，就获取了锁
  - 其他创建失败的，注册监听器监听这个锁
  - 释放锁，就是删除了这个节点，一旦释放掉就会通知客户端，然后有一个等待着的客户端可以再次重新创建

### Redis与Zookeeper分布式锁对比？
- redis 过程麻烦，不停重试，耗性能，增加网络拥塞；算法健壮性有待考证
- zookeeper 过程清晰，事件驱动

[Redlock 不可靠?](https://blog.csdn.net/chen_kkw/article/details/81433470)
Martin 举了个因为时间问题，Redlock 不可靠的例子。
1. client1 从 ABC 三个节点处申请到锁，DE由于网络原因请求没有到达
2. C节点的时钟往前推了，导致 lock 过期
3. client2 在CDE处获得了锁，AB由于网络原因请求未到达
4. 此时 client1 和 client2 都获得了锁

在 Redlock 官方文档中也提到了这个情况，不过是C崩溃的时候，Redlock 官方本身也是知道 Redlock 

### 分布式session怎么实现？
- tomat + redis 严重依赖web容器；早期使用
- spring + redis 优选

```xml
<Valve className="com.orangefunction.tomcat.rdissessions.RedisSessionHandlerValve"/>
<Manager className="com.orangefunction.tomcat.redissessions.RedisSessionManager"
    host="{redis.host}"
    port="{redis.port}"
    database="{redis.dbnum}"
    maxInactiveInterval="60"
    />
```

### 分布式事务？
单系统事务 -> 两阶段提交 -> TCC -> 本地消息表 -> 可靠消息最终一致性 -> 尽最大努力通知
#### XA事务
- TM 事务管理器
- RM 资源管理器

spring + JTA，这个方案，很少用，某一个应用内部跨多个数据库的操作，不符合微服务规范。
每个微服务只操作一个数据库，不允许直连其他数据库，只能通过调用其他微服务的接口。

#### TCC，业务上的2PC，业务重方案，实践太复杂
- Try 对各个服务的资源做检测以及对资源进行锁定或者预留
- Confirm
- Cancel

比较适合场景：一致性要求很高，核心系统的场景（支付、交易），需要编写大量的业务逻辑，判断一个事务的各个环节是否ok。

#### 本地消息表方案，缺点依赖本地的消息表
- 发起者 本地事务，插入业务表，插入消息表（后台线程轮询，待确认，重新发MQ）；之后发MQ；注册ZK，监听
- 接收MQ消息，先插入消息表（消息ID，保证幂等）,再处理业务；处理成功修改ZK

#### 可靠消息最终一致性方案
不要本地消息表，直接基于MQ；MQ要持久化保证可靠性；最终一致，不断重试
- 阶段一
  - 发起者，prepare消息到mq（此MQ不投递），mq返回，失败则返回
  - MQ返回成功，执行本地事务
  - 发送业务执行的结果到MQ
- 阶段二
  - MQ接收发起者的业务结果，业务结果失败，删除mq
  - 业务结果成功，投递MQ
  - 消费者接收消息，执行业务操作（一定保证幂等性，可以采用db唯一约束、redis标识、zk标识）
- 可靠性  
  - MQ会定时定时查看发起者，业务的最终状态，直到成功处理
  - MQ会保证消费者返回结果，直到消费者成功处理

发起者、消费者之间，还可以通过zk注册监听节点，让发送者获取消费者的消费状态。

```java
// 解决之道
Resutlt postMessage(Message, PostMessageCallback) {
    // 发送消息给消息中间件
    // 获取返回结果
    // 如果失败，返回失败
    
    // 进行业务操作
    // 获取业务操作结果
    
    // 发送业务操作结果给消息中间件
    // 返回处理结果
}
```

#### 最大努力通知方案？
最大努力通知服务，单独做一个服务，允许少数的分布式事务失败

发起者，直发一个消息，不管了。

建议：
- 严格的资金、交易、订单服务，选用TCC；其他可用可靠消息最终一致性方案，如扣减库存、会员积分、优惠券、商品信息
- 几百个服务，复杂的分布式大型系统，里面其实也没几个分布式事务
- 分布式系统太复杂10倍；性能太差、吞吐量低；一般只用A调用B，B调用C；
- 99%的分布式事务，做监控，记录日志，事后快速定位、排查、出解决方案、修复数据。

trade off，权衡，做分布式事务，成本高，复杂，开发时间长，性能和吞吐量下跌，容易出bug。

## 如何设计高并发系统架构？
高峰期
每秒100个请求，无压力；
每秒1000个请求，内存使用率上升，cpu负载高升；
每秒2000个请求，数据库快扛不住了，磁盘IO效率开始降低

业务拆分

缓存
读每秒5000请求，可以轻松几万/s的并发

MQ，异步
4000/s请求

分库分表，数据库读写分离

ES
单机版lucene
分布式elasticsearch，可以做几十亿的数据量

### 分库分表?
知道常见的中间件吗？
- 客户端中间件
  - TDDL
  - sharding-jdbc 当当开源；各系统依赖强，升级比较麻烦
- 代理中间件
  - cobar
  - atlas 360开源
  - mycat 基于cobar改造，

怎么拆分的？
- 表的垂直拆分，大表的很多字段（访问频率高的字段），拆分成小表的多个字段（访问率低的字段）
- 大表的数据，分成多个表存起来
- hash拆分 取模  扩容比较麻烦
- 范围拆分
  - 时间 最近数据比较集中
  - 空间

### 如何迁移分库分表？
停机分库分表
- 停机
- 后台临时程序，将老数据通过数据库中间件分发到新的库中
- 修改应用项目数据库连接配置，走新的数据库中间件
- 重启

缺点：
- 会几个小时停机
- 如果没有迁移成功，凌晨5点还不行，回滚，单库单表
- 第二天凌晨继续

数据量少的时候可行，数据量上亿，很困难。

不停机迁移
- 修改系统中所有写库的代码，新库老库**双写**
- 开发后台迁移工具，通过数据库中间件分发数据；不允许旧数据覆盖新数据
- 数据迁移工具，可能会跑几天
- 100%同步之后，修改系统中数据库配置

### 如何动态扩容缩容的分库分表？
一般过程
1. 选择数据库中间件，调用、学习、测试
2. 设计分库分表方案，分成多少库、多少表
3. 测试环境演练
4. 双写方案
5. 线上系统开始基于分库分表对外服务
6. 扩容了6个库之后，再继续扩

32个库，1024张表，假设每个表500W

改变数据库服务器的数量，而不用改变数据库和表的数量。
1. 设定好几台数据库服务器，每台服务器上几个库，每个库多少个表，推荐是32库 * 32表
2. 路由规则 orderId 取模 = 库， orderId / 32 模 32 = 表
3. 扩容的时候，申请增加更多的数据库服务器，安装mysql，倍数扩容，4台，8台，16台
4. dba负责将原先的数据库服务器的库，迁移到新的数据库服务器上去，迁移工具
5. 开发修改数据库连接配置
6. 发布上线，路由规则不用变

### 分库分表之后，全局id？
- 全局的一个库，生成主键的库，全局一个；简单，但是吞吐量不高；并发低，数据量大
- UUID 太长，不适合当主键，不利于B+Tree发挥作用，占用空间大
- 基于时间戳，获取当前时间 并发量高，会重复
- snowflake算法 twitter开源的分布式id生成算法，时间戳 + 机房ID + 机器ID +序列号
  一个64位的long型的id，1个bit是不用的（符号位），其中41bit作为毫秒数，10bit作为工作机器id，12bit作为序列号
  snowflake生成的ID整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞 。

```ts
符号 1bit               时间戳 41bit             机房ID 5 机器ID    序列号 12
0 - 0000000000 0000000000 0000000000 0000000000 0 - 00000 - 00000 - 000000000000
```
缺点：最多32个机房，每个机房32个机器，每个机房的每个机器每毫秒内4096个序列号

- redis分布式id生成
- mongodb的objectid
- Vesta  通用ID产生器，统一发号器，具有全局唯一性，大概有序，可反解，可制造，支持4种发布方式。

### mysql读写分离？主从同步延时？
读，什么情况下，缓存里没有数据？
- 缓存刚加上，并没有把数据库中的数据导入到缓存
- 缓存的内存满了，自动LRU，删除了一些

复制过程
- 主库 binary log
- 从库
  - io线程拉取，relay log
  - sql线程，重放
- 从库同步时，单线程的，串行，从库比主库慢，产生主从延迟。
- 主库写并发越高、从库延迟也回越高

主库写并发1000/s，从库的延迟会有几毫秒；
主库写并发2000/s，从库的延迟会有几十毫秒；
主库写并发4000/s，8000/s，从库好几秒，快死掉了。

主库宕机，**半同步机制**，主库返回成功的条件：本地binlog成功，并且有一个从库拉取relay日志成功。（对比RocketMQ，异步刷盘、同步复制）。

**并行复制**，SQL线程可以多线程并发执行，读取relay日志读取一个库的日志，重放。

要点：
1. 主从复制问题
2. 主从延迟问题
3. 主从复制数据丢失问题，半同步复制
4. 并行复制，多库并发重放relay日志，缓解主从延迟问题


如何解决mysql主从同步延时问题？
- show status Seconds_Behind_Master 查看从库落后主库几毫秒
- Java代码在主库上写入，由于主从复制延迟，立马查询了，然后更新失败了。
- 主从同步延迟严重？
  - 分库，同步延迟，意味着要分库分表了
  - 打开mysql支持的并行复制，多个库并行复制，不能根本上解决（并行复制意义不大）
  - 重写Java代码，插入之后，立马查询，这种操作有问题？
  - 如果确实存在必须插入，然后再查询，那么要直连主库。不过不推荐这样操作。

### 如何设计高可用架构？限流？降级？熔断？


## question
MQ、ES、Redis、Dubbo，
- 思考的问题
- 原理
- 生产环境的问题
- 系统设计（设计MQ、设计缓存，设计rpc框架）


|      |          | mq   | redis | dubbo | dist-tx | dist-lock | dist-db |
| :--- | :------- | :--- | :---- | :---- | :------ | :-------- | :------ |
| 可用 |          |      |       |       |         |           |         |
|      | 冗余     |      |       |       |         |           |         |
| 可靠 |          |      |       |       |         |           |         |
|      | 丢失     |      |       |       |         |           |         |
|      | 重复     |      |       |       |         |           |         |
|      | 持久化   |      |       |       |         |           |         |
|      | 优先级   |      |       |       |         |           |         |
| 集群 |          |      |       |       |         |           |         |
|      | 负载均衡 |      |       |       |         |           |         |
|      | 主从     |      |       |       |         |           |         |
|      | 协调者   |      |       |       |         |           |         |
|      | 复制     |      |       |       |         |           |         |
|      | 同步     |      |       |       |         |           |         |
|      | 并发     |      |       |       |         |           |         |
|      | 顺序     |      |       |       |         |           |         |
|      | 一致性   |      |       |       |         |           |         |
|      | 事务     |      |       |       |         |           |         |
|      | 伸缩     |      |       |       |         |           |         |
|      | 扩容     |      |       |       |         |           |         |
|      | 缩容     |      |       |       |         |           |         |
| 分布 |          |      |       |       |         |           |         |
|      | 分库     |      |       |       |         |           |         |
|      | 分表     |      |       |       |         |           |         |
|      | 切片     |      |       |       |         |           |         |
| 拥塞 |          |      |       |       |         |           |         |
|      | 积压     |      |       |       |         |           |         |
|      | 限流     |      |       |       |         |           |         |
|      | 降级     |      |       |       |         |           |         |
|      | 熔断     |      |       |       |         |           |         |
|      | 流量控制 |      |       |       |         |           |         |
